{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field\n",
    "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
    "\n",
    "The parameters of a `Field` specify how the data should be processed. We use the `TEXT` field to define how the review should be processed, and the `LABEL` field to process the sentiment. Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces.\n",
    "\n",
    "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels.\n",
    "\n",
    "`dtype:`This is because TorchText sets tensors to be LongTensors by default, however our criterion expects both inputs to be FloatTensors. Setting the dtype to be torch.float, did this for us. The alternative method of doing this would be to do the conversion inside the train function by passing batch.label.float() instad of batch.label to the criterion.\n",
    "\n",
    "All fields, by default, expect a sequence of words to come in, and they expect to build a mapping from the words to integers. If you are passing a field that is already numericalized by default and is not sequential, you should pass `use_vocab=False` and `sequential=False`.\n",
    "\n",
    "In addition to the keyword arguments mentioned above, the Field class also allows the user to specify `special tokens` (the unk_token for out-of-vocabulary words, the `pad_token` for padding, the `eos_token` for the end of a sentence, and an optional `init_token` for the start of the sentence), choose whether to make the `first dimension` the batch or the sequence (the first dimension is the sequence by default), and choose whether to allow the sequence lengths to be decided at runtime or decided in advance.\n",
    "\n",
    "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
    "\n",
    "We also set the random seeds for reproducibility.\n",
    "\n",
    "```\n",
    "    Field Attributes:\n",
    "        sequential: Whether the datatype represents sequential data. If False,\n",
    "            no tokenization is applied. Default: True.\n",
    "        use_vocab: Whether to use a Vocab object. If False, the data in this\n",
    "            field should already be numerical. Default: True.\n",
    "        init_token: A token that will be prepended to every example using this\n",
    "            field, or None for no initial token. Default: None.\n",
    "        eos_token: A token that will be appended to every example using this\n",
    "            field, or None for no end-of-sentence token. Default: None.\n",
    "        fix_length: A fixed length that all examples using this field will be\n",
    "            padded to, or None for flexible sequence lengths. Default: None.\n",
    "        tensor_type: The torch.Tensor class that represents a batch of examples\n",
    "            of this kind of data. Default: torch.LongTensor.\n",
    "        preprocessing: The Pipeline that will be applied to examples\n",
    "            using this field after tokenizing but before numericalizing. Many\n",
    "            Datasets replace this attribute with a custom preprocessor.\n",
    "            Default: None.\n",
    "        postprocessing: A Pipeline that will be applied to examples using\n",
    "            this field after numericalizing but before the numbers are turned\n",
    "            into a Tensor. The pipeline function takes the batch as a list,\n",
    "            the field's Vocab, and train (a bool).\n",
    "            Default: None.\n",
    "        lower: Whether to lowercase the text in this field. Default: False.\n",
    "        tokenize: The function used to tokenize strings using this field into\n",
    "            sequential examples. If \"spacy\", the SpaCy English tokenizer is\n",
    "            used. Default: str.split.\n",
    "        include_lengths: Whether to return a tuple of a padded minibatch and\n",
    "            a list containing the lengths of each examples, or just a padded\n",
    "            minibatch. Default: False.\n",
    "        batch_first: Whether to produce tensors with the batch dimension first.\n",
    "            Default: False.\n",
    "        pad_token: The string token used as padding. Default: \"<pad>\".\n",
    "        unk_token: The string token used to represent OOV words. Default: \"<unk>\".\n",
    "        pad_first: Do the padding of the sequence at the beginning. Default: False.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', batch_first=True)\n",
    "LABEL = data.LabelField(sequential=False, dtype = torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP). \n",
    "\n",
    "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as `torchtext.datasets` objects. It process the data using the `Fields` we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review.\n",
    "\n",
    "By default this splits 70/30, however by passing a `split_ratio` argument, we can change the ratio of the split, i.e. a `split_ratio` of 0.8 would mean 80% of the examples make up the training set and 20% make up the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED), split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalize\n",
    "Next, we have to build a vocabulary. This is a effectively a look up table where every unique word in your data set has a corresponding index (an integer). We do this as our machine learning model cannot operate on strings, only numbers. What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special unknown or <unk> token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I <unk> it\".\n",
    "\n",
    "The following builds the vocabulary, only keeping the most common max_size tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 232441), (',', 221500), ('.', 189857), ('and', 125689), ('a', 125534), ('of', 115489), ('to', 107557), ('is', 87581), ('in', 70216), ('I', 62190), ('it', 61399), ('that', 56406), ('\"', 50836), (\"'s\", 49934), ('this', 48383), ('-', 42634), ('/><br', 40664), ('was', 39911), ('as', 34825), ('with', 34402)]\n",
      " \n",
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
      " \n",
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print(\" \")\n",
    "print(TEXT.vocab.itos[:10])\n",
    "print(\" \")\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataIterator\n",
    "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration. We'll use a BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
    "\n",
    "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using torch.device, we then pass this device to the iterator.\n",
    "\n",
    "In torchvision and PyTorch, the processing and batching of data is handled by DataLoaders. For some reason, torchtext has renamed the objects that do the exact same thing to Iterators. The basic functionality is the same, but Iterators, as we will see, have some convenient functionality that is unique to NLP.The BucketIterator is one of the most powerful features of torchtext. It automatically shuffles and buckets the input sequences into sequences of similar length.\n",
    "\n",
    "\n",
    "| Name        | Description           | Use Case  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Iterator      | Iterates over the data in the order of the dataset. |  Test data, or any other data where the order is important. |\n",
    "| BucketIterator | Buckets sequences of similar lengths together.\t      |   Text classification, sequence tagging, etc. (use cases where the input is of variable length) |\n",
    "| BPTTIterator | An iterator built especially for language modeling that also generates the input sequence delayed by one timestep. It also varies the BPTT (backpropagation through time) length. This iterator deserves its own post, so I'll omit the details here. |    Language modeling |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = 64,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 64]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 64x1084 (GPU 0)]\n",
       "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iterator.__iter__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text size: [sentence length, batch size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.text:\n",
      "tensor([[ 7223,  3035,     5,  ...,     1,     1,     1],\n",
      "        [  146,   146, 13551,  ...,     1,     1,     1],\n",
      "        [   11,    35,   233,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [ 3131,  4152,  2568,  ...,     1,     1,     1],\n",
      "        [   11,    57,    29,  ...,     1,     1,     1],\n",
      "        [  323,    11,    19,  ...,     1,     1,     1]], device='cuda:0')\n",
      "batch.text.size:\n",
      "torch.Size([64, 1021])\n",
      "batch.label:\n",
      "tensor([0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0') 64\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(\"batch.text:\")\n",
    "    print(batch.text)\n",
    "    print(\"batch.text.size:\")\n",
    "    print(batch.text.size())\n",
    "    print(\"batch.label:\")\n",
    "    print(batch.label, len(batch.label))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed above data into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
